# -*- coding: utf-8 -*-
"""RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K9RYic7MSzUCspjk-FEOEMCFYsM1Lbjr
"""

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from transformers import AutoTokenizer, AutoModel
import networkx as nx
import torch

class RAGFramework:
    def __init__(self, esg_model_name="yiyanghkust/finbert-esg"):
        # Load the ESG-related pre-trained model
        self.tokenizer = AutoTokenizer.from_pretrained(esg_model_name)
        self.model = AutoModel.from_pretrained(esg_model_name)
        self.knowledge_graphs = []
        self.esg_scores = []

    @staticmethod
    def cosine_similarity_sklearn(embedding1, embedding2):
        """
        Computes the cosine similarity between two embeddings using sklearn.
        """
        embedding1 = np.asarray(embedding1).reshape(1, -1)
        embedding2 = np.asarray(embedding2).reshape(1, -1)
        return cosine_similarity(embedding1, embedding2)[0][0]

    def get_esg_embedding(self, text_chunk):
        """
        Extracts embedding for a given ESG-related text chunk.
        """
        inputs = self.tokenizer(text_chunk, return_tensors='pt', truncation=True, max_length=512)
        with torch.no_grad():
            outputs = self.model(**inputs)
        cls_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()
        return cls_embedding

    def add_corpus(self, graph, text_chunk, esg_score):
        """
        Adds a knowledge graph and its ESG score to the corpus with embeddings.
        """
        embedding = self.get_esg_embedding(text_chunk)
        self.knowledge_graphs.append((graph, embedding))
        self.esg_scores.append(esg_score)

    def compute_similarity(self, embedding1, embedding2):
        """
        Compute cosine similarity between two text embeddings.
        """
        return self.cosine_similarity_sklearn(embedding1, embedding2)

    def find_top_matches(self, input_graph, input_embedding, top_n=5):
        """
        Finds top N matches for a given input graph and its embedding.
        """
        similarities = []
        for i, (corpus_graph, corpus_embedding) in enumerate(self.knowledge_graphs):
            similarity = self.compute_similarity(input_embedding, corpus_embedding)
            similarities.append((i, similarity))
        return sorted(similarities, key=lambda x: x[1], reverse=True)[:top_n]

    def compute_weighted_esg(self, input_graph, input_text):
        """
        Computes weighted ESG score based on top matches.
        """
        input_embedding = self.get_esg_embedding(input_text)
        top_matches = self.find_top_matches(input_graph, input_embedding, top_n=5)
        total_similarity = sum(similarity for _, similarity in top_matches)

        weighted_esg_score = sum(
            (self.esg_scores[idx] * similarity) / total_similarity
            for idx, similarity in top_matches
        )
        return weighted_esg_score